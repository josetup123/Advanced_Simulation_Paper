\relax 
\citation{nations_as_nodate}
\citation{Patrick Baylis}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{}\protected@file@percent }
\newlabel{introduction}{{I}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Literature Review}{1}{}\protected@file@percent }
\newlabel{Literature Review}{{II}{1}}
\citation{haksar_distributed_2018}
\@writefile{toc}{\contentsline {section}{\numberline {III}Architecture and Information Collection}{2}{}\protected@file@percent }
\newlabel{Architecture and Information Collection}{{III}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Problem Description}{2}{}\protected@file@percent }
\newlabel{Problem Description}{{IV}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-A}}Stage 1}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-B}}Stage 2}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {V}Methodology}{2}{}\protected@file@percent }
\newlabel{Methodology}{{V}{2}}
\newlabel{Pseudo Heuristic - Optimization Approach}{{V}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  System Architecture: The illustration depicts the fundamental components of this digital twin. Initially, satellite imaging is employed from three sources—Landsat, MODIS, and VIIRS. Each dataset is standardized, offering a confidence level for the likelihood of a fire event at each represented heat point in the future. This data is presented in tabular format and forms the basis for the backend, comprising a Django-based API and a containerized MySQL platform. The API operates as a RESTful interface for socket communication and serves as an executor of operations within the inner backend. Real-time traffic information is sourced from TomTom live data, communicating with the OSRM routing engine to generate a realistic cost matrix for transportation in optimizing neighboring station placement. Following optimization, the results are visualized in the AnyLogic GUI. The second stage involves a reinforcement learning-based policy aiming to select the policy that minimizes the economic impact caused by a fire event. An advantage actor-critic is utilized to train independent clusters for each agent, empowering them to decide on the necessary containment actions for fire events. }}{3}{}\protected@file@percent }
\newlabel{fig:arch}{{1}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Experimentation}{3}{}\protected@file@percent }
\newlabel{Experimentation}{{VI}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {VI-A}}Data Sources}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {VI-B}}Neighboring Station}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {VI-C}}Fire Containment}{4}{}\protected@file@percent }
\newlabel{Fire Containment}{{\mbox  {VI-C}}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Deep Reinformcement Leanring Parameters: The experimental parameters are configured as follows: \texttt  {EPISODES=50000} dictates the number of agent simulations for policy improvement; \texttt  {DISCOUNT\_FACTOR=1.0} emphasizes the full consideration of future rewards; \texttt  {seed\_value = 42} ensures reproducibility through a set seed for the random number generator. \texttt  {num\_states = Max\_FirePoints} signifies the count of environmental states, often representing diverse fire scenarios or locations. \texttt  {num\_plans = 2} denotes the available action choices for the firefighter, likely relating to deciding whether to address a fire. \texttt  {T = 20} defines the time horizon, indicating the number of time steps before updating policies or the simulation duration. \texttt  {success\_pr = [0.0, 0.5, 0.9]} characterizes the success probability of extinguishing a fire, with values indicating varying success levels. The \texttt  {degrade\_pr} matrix portrays degradation probabilities for each state, reflecting the likelihood of environmental deterioration. \texttt  {C\_reward = 10} denotes the reward for fire prevention per time step, incentivizing a safe environment. \texttt  {C\_Operating} encapsulates operating costs, suggesting state-dependent variations. \texttt  {C\_Putoff} captures put off costs, contingent on the chosen action, likely reflecting the expenses associated with fire suppression strategies. }}{4}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Stages Transfering: We illustrate the process of transitioning from the initial stage to the subsequent deep reinforcement learning (DRL) model in a two-stage framework. Initially, our Neighboring approach strategically allocates the requisite number of fire stations to address the prevailing number of fire events in a given area. The fire stations surrounding each fire event are then clustered, constituting Stage two. In this stage, the individual fire stations and associated resources are treated as a unified entity. The DRL approach subsequently formulates a policy, determining the optimal actions for each fire event—whether to contain, extinguish, or withhold resource allocation—at discrete time intervals, starting at $t=0$. Visualized through three arrows, these actions represent the various states the agent (comprising a cluster of fire stations and resources) can assume during the second stage. Our temporal horizon spans 20 time steps, with each step equivalent to an hour in Anylogic units, providing a comprehensive representation of the evolving dynamics.}}{4}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Actor Loss: It indicates the cost of training the policy network. The actor loss leads policy modification to raise the likelihood of actions leading to better returns and decrease the likelihood of actions leading to lower returns. It is a component of the overall loss function, which combines actor and critic losses with the goal of maximizing the predicted cumulative reward by changing both the policy and value functions. }}{4}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Critic Loss: In Advantage Actor-Critic (A2C), the critic loss indicates the difference between the projected state value and the actual return seen during reinforcement learning training. The critic is in charge of estimating the state-value function, which calculates the predicted cumulative reward from a particular condition in A2C. The critic loss, which is frequently expressed as the mean squared error between the anticipated state value and the actual return, is a measure of how well the critic's predictions match the genuine rewards received in the environment.}}{4}{}\protected@file@percent }
\bibstyle{IEEEtran}
\bibdata{references.bib}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces This section elucidates the profound impact of actions on the overarching policy return. A well-converged model is anticipated to showcase a discernibly smoother pattern, contrasting with the inherent noise observed during earlier stages. The graphical representation serves to illustrate the pivotal role actions play in shaping the cumulative return of the policy, offering insights into the stability and effectiveness achieved through convergence.}}{5}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Action Schema: This comprehensive depiction provides a detailed examination of state rewards, actions, and individual states. To navigate the intricacies of the data, it is advisable to implement a state-action filter to prevent unnecessary traversal through events where fires have already been extinguished. Notably, an observable pattern emerges, highlighting the effective control measures applied to fire event 3, also denoted as state 3, which demonstrate a successful containment strategy deployed multiple times.}}{5}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}